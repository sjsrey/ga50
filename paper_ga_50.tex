% Created 2018-12-06 Thu 10:15
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
%\usepackage{minted}
%\author{Sergio Rey}
\date{\today}
\title{Geographical Analysis: Reflections of a Recovering Editor}
\hypersetup{
 pdfauthor={Sergio Rey},
 pdftitle={Geographical Analysis: Reflections of a Recovering Editor},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.1.1 (Org mode 9.1.6)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section{Introduction}
\label{sec:org79c12a3}

In this paper, I consider both the past and future of Geographical Analysis (GA)
the journal, as well as the broader academic field. I first give an overview of
how my career path has intersected with both the journal and the field of
spatial analysis. This is followed by a section that covers three external
trends that I think will form the backdrop for the future evolution of spatial
analysis and GA. These include the rise of artificial intelligence, machine
learning, and data science as disciplines, as well as some structural changes in
the organization of science and the appearance of competing models for that
organization. The third large-scale trend that we will intersect with has to do
with the democratization of spatial analysis and of spatial data. After covering
these trends, I then move onto areas that I think have been under-studied, but
are likely to be important areas for future research in spatial analysis. These
concern multi-scalar integration and the problem of endogenous spatial units. In
the final section of the paper, I identify future opportunities for GA.

\section{History}
\label{sec:orgfb43d5b}
To provide some context, I first discuss my history and association with GA and spatial analysis in general. I was a graduate student at UCSB in
geography in the early 1990s, where I obtained both my masters and doctoral
degrees. Looking back now, I realize just how fortunate I was to do graduate studies
there, as it was a period of substantial growth for the department, and one when
GIScience was coming to the fore as a field.

The journal and the department seemed to be joined at the hip back then. GA was
the premier outlet for advanced theoretical research in quantitative geography.
The department had, by that time, established itself as one of the world's
leading spatial analysis centers. Two of the former editors of the journal, Reg
Golledge and Mike Goodchild, were faculty at UCSB during this time. It is also
with some personal pride to note that the department would also produce another
future editor of GA: Alan Murray.

One of my first papers appeared in GA, and I still remember the feeling of
excitement in seeing my name in print in the table of contents. I don't think I
am alone in my high regard for the journal as I've heard from many colleagues
that they view GA as the flagship journal for the best in quantitative spatial
analysis. The high regard can sometimes reflect strongly held personal
interactions with the journal, and it is here where I want to reflect on a
painful, but important, piece of history.

I have always felt that you can measure the health of a journal in the strength
of its community. Scholars associate themselves with a
journal through publishing their best work there, contributing as
editorial board members and referees, as well as serving on the executive
committee or as editor. I was honored to join the editorial board in 1996, and
very much enjoyed the opportunity to read some of the latest in spatial analysis
and provide suggestions and referee reports for the editor.

Twelve years on, I resigned from the editorial board over what I felt was a
broken governance model that was treating members of the community poorly. The
reaction was swift. I received numerous admonishments from senior colleagues for
the damage they feared such a move would cause to the journal. Other senior
colleagues quickly followed my action and also resigned. This turmoil was
unfortunate; however, I felt I did the right thing by resigning.

With hindsight, I have learned two lessons from this episode. First, time can
indeed heal wounds. Second, the high regard members of our community felt for
the journal, and honest disagreements about how the journal should function are
healthy signs. As evidence of the healing power of time, in 2013, I
was approached by the executive board to consider becoming the editor. I report
this episode because I think it reflects the maturity of the GA community in that
disagreements can arise when people feel strongly about an issue. How they
resolve those disagreements says a lot about the health of the community.

I served a three-year term at GA (2014-2017), and had I not preceded this with
a 16-year term at the International Regional Science Review (IRSR), I may have
sought a second term. I mention this here not to self-promote but, instead, to
say that the differences between the two journals was marked. At GA, I experienced a much more engaged editorial board.
Numerous colleagues offered constructive criticisms and support, beyond referee
reports, to me during my term as editor. These reflected a genuine concern for
GA and a shared sense of responsibility.

\section{Emerging Trends}
\label{sec:orga8cc385}

Although prognostications about the future are always dangerous, I see three
prominent emerging trends that will likely shape the future of our field and GA: The rise of machine learning, artificial
intelligence and data science; the emergence of the competing models of
spatial analysis; and the democratization of spatial data and analysis.



\subsection{The Rise of Machine Learning, Artificial Intelligence, and Data Science}
\label{sec:org2d89c2b}

The turn of the century has witnessed an explosion of interest in the fields of
machine learning, artificial intelligence, and data science. This interest has
generated an abundance of applications of these technologies and methods across
a wide array of substantive domains. More importantly, it has raised fundamental
epistemological issues for traditional disciplines - geography and spatial
analysis have been somewhat isolated from these events, although I expect this
to intercede more fully in the coming years.

One main issue that sits before geographical analysis is in the realm of
the Big Data era. Here the often quoted phrase is that data is the new oil.
Companies like Google and Facebook and Twitter have mined massive amounts of
data sources, and so-called data lakes, to develop services such as search and
social graphs that have transformed our world. It is true that these
new forms of data have been critical to the success of these companies. But, I
would argue that if data are the new oil, and I believe this is true, then
analytics is the new refinery. And, it is here where I see fantastic
opportunities for spatial analysis.

The widespread availability of spatial web-based technologies has lowered the
cost of entry to anyone wanting to develop mashups surrounding mapping and other
types of visualizations. As a result, there has been an explosion of such
applications. At the same time, however, one gets the feeling that we may be
hitting diminishing returns to these efforts. Visualization leads to natural
questions about what processes are responsible for the patterns that these
innovative visualizations are uncovering. Answering those questions is the
bread-and-butter of quantitative geographic analysis. However, engaging with the
world of web-mashups for spatial data is something that we are not capturing
or realizing at this moment as a community. Analytics is the mechanism to turn
visualizations into information and knowledge, and spatial analytics, in
particular, is the next generation refinery for the big data realm.

Computational thinking \cite{Barba_2016} is a second area that the new data
science challenge offers to move spatial analysis forward. Computational
thinking gives us new ways to engage students with advanced concepts. By
beginning with the final solution to a problem, say a location-allocation
problem or spatial search problem, and only then unveiling the solution through
the details of the algorithm, students feel a sense of excitement about why the
underlying fundamentals are important, and because of that recognition, engage
with those fundamentals in a more profound sense, thus lowering the bar to
advancing their skill sets.

Adopting computational thinking as a pedagogical strategy helps students
acquire the mindset that when facing a challenging problem that appears
daunting, they can re-frame that to see the learning process in this particular
case has a slow feedback loop. That is, they recognize they will eventually
learn these concepts, it just takes longer when dealing with advanced concepts
relative to basic concepts. That recognition lowers the frustration level
as the expectations align with reality. A subtle, but important,
shift in how we present material can encourage students to become
lifelong learners.

To facilitate computational thinking, we can lean on some fantastic innovations
in scientific computing in reshaping the teaching of spatial analysis. Groups
such as Software Carpentry and Data Carpentry
\footnote{\url{https://carpentries.org}} have popularized, and refined, the
boot-camp model to give faculty, students, and staff of educational institutions
primers on scientific scripting. Data carpentries tailor lessons at particular
domains such as bioinformatics, atmospheric, and ocean sciences. These are often
very successful in giving a shot in the arm to the participant in getting them
so-called over the computational hump.

As a community, academic spatial analysis has been slow to engage with these
pedagogical transformations. A cursory examination of GA contents shows scant
coverage of the teaching of quantitative spatial analysis. I find this troubling
for two reasons. First, the lack of attention to spatial analysis pedagogy in GA
reflects the historical realities  that have prioritized research over teaching in
the field of spatial analysis. While understandable, this is a failure on
the journal's part. By neglecting pedagogy, GA removed itself from the training
of the future generations of theoretical quantitative geographers.

The second reason I find this neglect troubling is that academic spatial
analysis is missing an opportunity to contribute to, and shape, the widespread
adoption of spatial methods out in the wild. Increasingly at computationally
oriented conferences I attend, and from reading blogs of high profile data
scientists, I see the rise of what I will term as ``Code Hubris''. By this, I
mean the attitude that by learning basic principles of software development and
general data science, one becomes an authority on the applications of these
methods to domains outside one's training. As they seek to conquer the world,
newly minted data scientists are ignoring the hard-learned lessons about the
unique nature of geographic data that academic quantitative geographers have to
offer. As a result, their data science is repeating mistakes we made decades ago
\cite{Arribas_Bel_2018}.

GA will have to decide how to position itself in the era of big data and machine
learning. We can view GA's historical roots in theory \cite{Golledge_2010}  as either a burden or a strength in this regard. The first perspective
reflects the dominant thinking in the machine learning community in that the
goal is prediction, and not understanding the causal processes underlying the
patterns that have been recognized by the latest algorithm. Any machine learning
researcher worth their salt is open about this and does not over-extend by
claiming that these approaches yield causal insights. Some, however, have gone
beyond this measured perspective to argue that theory is no longer needed as the
growing power of machine learning prediction will be
sufficient\cite{anserson_2006_eot}.

I think this is overblown and misplaced for two reasons. First, the tension
between pattern and process is artificial. One cannot separate consideration of
pattern from questions about processes that may be at work under the hood.
Second, seeing predictive methods as orthogonal to causal inquiry shows a lack
of imagination. I view the two sets of methods as complements, not substitutes,
and by leveraging that relationship, we can be on a path to better science.
By better science here, I mean an accelerated innovation cycle where we go
from pattern discovery to posit causal mechanisms in a much quicker cadence than
we would if we have separated the researchers working on exploratory methods
from those working on process. The new spirit of computational thinking is
blurring the distinction between the traditional spheres of investigation.


\subsection{Cathedrals or Bazaars}
\label{sec:orgbb69af0}
A related trend I can foresee is a competition between different models for
organizing the field of spatial analysis. Here I think Eric Raymond's Cathedral
and the Bazaar \cite{raymond_cathedral_1999-2} offers the appropriate
metaphors for contrasting what I think are alternative models competing for the
future spatial analysis. In the Cathedral model, large companies control the
distribution of spatial analysis software. Many researchers turn to esri
software and think that it is the final arbiter of what to use when carrying out
spatial analysis. Simplification of the choice of tool to use is desirable.
However, it has costs as well. The primary costs are in the centralization of
the development model and vendor lock-in.

The alternative is the bazaar model in which no central authority exists.
Instead, structure emerges organically through institutions and organizations
working collaboratively on the code base.
GitHub\footnote{\url{https://github.com}} is particularly interesting as it has
become the place where developers post open source code to tap into the
decentralized development model that git affords. This open source model has
advantages in terms of tapping into network effects and having the coordination
implicitly built into the system.


It is not clear which of these two models will come to dominate spatial analysis
in the future. There are some arguments that open source has already lost the
battle in that large companies, such as Google, have co-opted the open-source
model, and in clever ways centralized the rewards of that model to produce
profitable products\cite{Mark_2018}. For example, their web search uses Python
and related packages under the hood to produce its web services. Critics have
claimed Google has reaped vastly more benefits than it has returned to the
community. Whether these are fair criticisms is subject to debate. On the
benefits side, I would be remiss, however, not to mention the contributions
Google has made to spatial analysis. Through their summer of code project, Google
has supported thousands of
student developers  on open source projects, many of them in
the spatial analysis domain (e.g., PySAL, osgeo).

Google is not the only company using open source to support its business model.
Agile companies such as Mapbox and Carto  have taken a more
collaborative stance towards open source projects to build new products around
open source technologies. The question is whether spatial analysis software
becomes controlled by a spatial analytics industrial complex (i.e., the
Cathedral model) or will take the form of the more distributed organization
(i.e., the bazaar model).

\subsection{Democratization of Spatial Data and Analysis}
\label{sec:org3bf5ccc}
The third development in the future of geographical analysis will be in the area of
the democratization of analysis and data. The open source movement is driving the practice of making the source code underlying a spatial analytical method publicly
available for the reading. The other aspect of ``open'' is that the source code
is available for modification and extension. Both of these freedoms are critical
to fueling growth in the number of people who can engage with spatial analysis
methods and, ultimately, the increase in the advances in the form of new
methods.\footnote{A referee astutely pointed out that 
this recent engagement of spatial
analysis with the open source movement has largely ignored earlier criticisms of
GIS for its technocratic elitism\cite{obermeyer_1995} and calls for a
more open GIS \cite{pickles_1995}.}


Facilitating that openness will be increasingly important to maintain and
enhance these channels of innovation \cite{rey_show_2009}. As I touched on
previously, it is not at all clear if the open source model will be the dominant
paradigm. We will soon have to address challenges arising on the horizon.

The democratization of open data is an equally important development to the
democratization of analysis. Currently, there is a developing \emph{spatial
data provision industrial complex} and that is doing quite well. Companies
\cite{ncdb_2018} as well as research groups \cite{ipums_2018,Logan_2014} have
taken publicly-available data in the form of the census and American Community
Survey data, in the US case, and added value through easy to deliver front-ends
providing one-stop-shopping to researchers looking for accessible data. These
companies and groups offer valuable services to the research community and
have supported thousands of empirical studies.

While impressive in their impact and scope of use, this model is not without
problems. Many prominent data providers charge a subscription, and this seems
logical given the investment in time and effort put into building and
maintaining the services. However, it does present a constraint for
resource-strapped individual scholars. The business model that has arisen to
relax this constraint is through subscription fees for libraries which transfer
the cost away from the individual to the institution.

There is a second constraint in these use of these types of data services.
Often the licensing agreement places restrictions on how the data, once
accessed, can be used. Researchers may not redistribute the data without prior
permission of these providers, which represents a serious, under-recognized,
constraint on scientific innovation preventing further community enhancements
to the databases.


I would argue that relaxing the restriction on downstream redistribution of the
data would not necessarily prevent future resources from coming into the project
\cite{rey_open_2014}. An analogy here would be to look at the grandfather Linux
distributions such as Debian\footnote{\url{http://debian.org}} released for free
under open source licenses. Entire companies, such as
Canonical\footnote{\url{http://www.canonical.com}}, have been built around
packaging that software and providing services for its application. Moreover,
this has not dampened the growth of the original Debian project. Mechanisms to
support the project have included consulting fees by private companies who want
specialized applications, and support from governments who see the value in what
the grandparent distribution is providing. Finally, we should not ignore the
contributions of members in the community who also value the importance and
availability of the original distributions.


The type of model used by leading data providers in the spatial domain can
enhance the impact that their efforts are having on research productivity as
well as maintain the financial viability of those projects. The trade-off
between the benefits that the data providers offer to the community versus
the constraints they place on downstream redistribution of the data represent a
trade-off between data use and data innovation. A subtle, but important, tweak of
the system could increase the importance of innovations flowing from these
projects by a rethinking of the business models that underlie these data 
provision activities.

A final aspect to the democratization of both spatial analysis methods and data
relates to the incentives individuals have to participate. Historically, it has
been notoriously difficult to get funding from federal agencies for both open
source tools development and their provision to the community. We have recently
seen signs of change with substantial investments by the US National Science
Foundation in both of these areas. Here we have the government innovating and
leading the charge and academia playing catch-up. By catch-up, I mean these
types of contributions (data provision and scientific software development) have
historically been undervalued, but are now being considered in the formal
evaluation of academic researchers \cite{eghbal2016roads}.

\section{Opportunities}
\label{sec:org000df0f}
Given these emerging trends, I can see opportunities for both the field of
geographical analysis as wells as for GA the journal.
\subsection{Opportunities for Geographical Analysis: The Field}
\label{sec:org52b49a6}
There are two areas of research that I feel are important future
opportunities for spatial analysis to contribute significant advances. The
first has to do with what I call multiscalar integration and the second
concerns the problem of endogenous spatial units.

\textbf{Multiscalar Integration}: Geographical analysis is fragmented methodologically
in many ways but, in particular, from a scalar perspective. By this I mean we
have excellent work  on micro-scale models. These include agent-based
models, cellular automata, and microsimulation models. There are also
researchers who investigate more macro-oriented models such as integrated
multi-regional econometric and input-output models, and the widespread use of
spatial econometrics models  focusing on areal units as the
organizational frame. We might also consider work at the meso-scale where
the grouping observations from micro-perspective to identify clusters
as constellations of observations.

We have individuals doing innovative work at these different spatial scales.
However, if you attend our meetings, or read through our literature, there is
precious little cross-fertilization between these three groups of studies. This
is unfortunate because it misses the opportunity to strengthen these areas of
study.

There is a pressing need for integration across the scales: or what I refer to
as multi-scalar integration. Such integration will lead to better spatial
science in two regards. First, the comprehensive modeling frameworks that will
arise from addressing integration will have a much wider scope in the types of
problems they can address relative to the piecemeal application of the
individual-scale models. Second, even if one wants to remain in the domain of a
particular scale, viewing the associated modeling frameworks for that scale
from the perspective of a larger integrative context will enhance the development
of the individual components. By placing their work in a broader framework,
researchers will be forced to consider more dimensions of their problems.
Simply put, what was an exogenous variable when looked at from the isolation of
a microscale perspective may become an endogenous variable from an integrated
perspective. We can say the same for the meso- and macro-scales.

When one adopts an integrated multiscalar perspective, accuracy concepts also
become expanded. Rather than focusing on the partitive accuracy of a particular
scale-specific modeling framework, the concern shifts towards holistic accuracy.
Issues of error propagation and reverberation across components at different
scales become central research topics.


\textbf{Endogenous spatial units}: The second promising area for advances in spatial
analytical research has to do with the problem of endogenous boundaries. By and
large, our current modeling frameworks in spatial analysis take space and its
representation as fixed in a container sense, and we study how attribute
distributions evolve over those fix units. Examples can be seen in the work on
regional labor markets as well as housing markets where administrative
boundaries are often used to record the data under study with the assumption
that those administrative boundaries coincide with the boundaries of the actual
market process. In practice, there is reason to suspect these assumptions are
restrictive. The notions of growth and change suggest that it is not only the
number of people in the labor market that changes over time but also the spatial
extent of the market. This change would violate the common assumption of fixed
administrative boundaries as useful proxies for process boundaries.

Similarly, in work on neighborhood effects and neighborhood dynamics, often
census tracts and other administrative units are used to define neighborhoods,
either explicitly as the neighborhood unit, or by aggregation to combine tracts
that have a similar socioeconomic composition and geographical connectivity.
Once these neighborhood boundaries are defined, the focus shifts to the
socioeconomic composition of the individuals and households who reside within
the boundaries and how that composition may fluctuate over time. However, again, the
boundaries themselves are fixed and do not evolve in any real sense. What can
evolve here, to be explicit, is that the neighborhood labels applied to a
given tract as the label for the neighborhood type it is assigned. The
boundaries of the primitive units are not allowed to change, however.

Endogenous boundaries are thus an important area for future research. We have
good reason to suspect that neighborhoods change not just in their composition
but also spatial extent, and the same holds for housing and labor markets. The
challenge is how to incorporate boundary change into our analytical frameworks.
We might be able to draw inspiration from our colleagues in the physical
sciences who model things such as storms and fires as systems that have both
compositional and spatial dynamics \cite{yuan2001representing}.

\subsection{Opportunities for Geographical Analysis: The Journal}
\label{sec:org01c95fd}


Given the challenges facing the future of spatial analysis, as well as the
opportunities they can represent, it is interesting to think about
what the GA as a journal can do to situate itself in the future. There are
tensions between deciding whether to rebrand the journal and modernize in terms
of the trends happening in the data science, machine learning, and artificial
intelligence era and their articulation in spatial analysis. On the other hand,
the journal has a well-established brand that has been hard-earned through 
decades of effort by dedicated editors and editorial board members. That
branding is vital as reputations of journals are a key currency in the
academic labor market. The rankings of journals matter for the evaluation of
promotion cases and, as we have seen, that process gives scholars guidance and
incentive to decide how to allocate their research  efforts.

Journals thus can have a crucial influence in shaping the direction of a field.
However, that capability is a function of the currency of the journal. GA's
brand is in its by-line "A theoretical journal in quantitative geographical
analysis". Theory has been the distinguishing focus of the journal relative to
the other major geographical outlets. In the future rebranding of the journal, we
need to be conscious of the tenuous status theory occupies in the new data science
era. We have seen that theory offers some opportunities for engagement with
machine learning and artificial intelligence that, to date, have been ignored at
best, or, at worst, dismissed as irrelevant. If GA were to serve as the forum
for investigating the new role of theory and geographical analysis in the modern
era I think it would do a significant service for the community.

Branding, or rebranding, the journal will not be the only consideration GA faces
in the future. Scientific publishing is undergoing fundamental changes in both
the underlying technology used to produce scientific articles, as well as in the
organization of the market and the engagement of scholars. Within spatial
science, we have been slow to tap into the opportunities that these technological
and organizational shifts offer. The dead tree journal article is very likely to
become a relic of the past as new forms of digital publication become viable.
Innovations such as the Jupyter notebooks \footnote{\url{https://jupyter.org}}
allow for powerful dissemination mechanisms integrating the narrative text of an
article  with the source code, visualisations, and other forms
of media to present a much more comprehensive and reproducible depiction of the
research effort.

Moreover, by tapping into the technology of software code repositories, the
research artifact can become a living document in the sense that the source code
is fully available to the researchers who want to build on the contribution.
There are fantastic opportunities here to hard-wire attribution networks and
build a genuine democratic approach towards science.

There have been some innovative journals in GIScience that have started to adopt
new models of production. The Journal of Spatial Information Science is fully
Open Access and makes its content available gratis to the world. Its status
derives from an editorial board of highly-regarded GIscientists. Interestingly,
many of the members of these boards also serve on the editorial boards of
commercial journals so, evidently, the two models can co-exist as far as the
individuals are concerned. GA may want to consider these alternative models, in
terms of the structure and content, and how we deliver the journal.

Having served for one term as editor of GA I, know how challenging it can be to
think about, let alone try to implement, some of these structural changes on top
of the normal duties facing an editor. It is not rational to place this all on
the shoulders of the editor, but really should be a distributed effort that
engages all contributors to the journal including authors, referees, and,
hopefully, innovative scientific publishers. These are significant challenges,
but if we can rise to meet them, we will generate a better spatial science for
it.

\bibliography{ga50}
\bibliographystyle{unsrt}
\end{document}
